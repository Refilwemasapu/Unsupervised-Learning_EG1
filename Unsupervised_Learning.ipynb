{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 30px\">\n",
    "<font color='blue'> <b>Personalized Anime Recommendations for 2024</b></font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"javascript:code_toggle()\"><img src=\"https://blog.playstation.com/tachyon/2016/10/unnamed-file-6.jpg\" width=\"1000\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "*  [ Project Overview](#chapter1)\n",
    "    *  [1.1 Introduction](#section1_1)\n",
    "    *  [1.2 Aim](#section1_2)\n",
    "    *  [1.3 Objectives](#section1_3)\n",
    "*  [ Importing Packages](#chapter2)\n",
    "*  [ Loading Data](#chapter3)\n",
    "*  [ Data Cleaning](#chapter4)\n",
    "*  [ Data Preprocessing](#chapter5)\n",
    "*  [ Exploratory data analysis (EDA)](#chapter6)\n",
    "*  [ Model Training and Evaluation](#chapter7)\n",
    "*  [ Model parameters & hyperparameters tunning](#chapter8)\n",
    "*  [ Conclusion](#chapter9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview <a id=\"chapter1\"><a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Introduction <a id=\"section1_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Aim<a id=\"section1_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Objectives<a id=\"section1_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Packages <a id=\"chapter2\"><a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Importing Packages</b> is an essential step in any data analysis or machine learning project, as it allows you to access and utilize various libraries and tools to perform specific tasks. Python provides a rich ecosystem of packages for data manipulation, visualization, statistical analysis, and machine learning. Commonly used packages include pandas for data manipulation, matplotlib and seaborn for data visualization, scikit-learn for machine learning, and numpy for numerical computations. Importing the right packages at the beginning of your analysis ensures that you have the necessary tools to effectively explore and analyze your data.\n",
    "</di>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data <a id=\"chapter3\"><a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Loading Data</b> is the initial step in the data analysis process, involving the retrieval and loading of data into a suitable format for analysis. This step is crucial as it sets the foundation for the entire analysis process. The data can be loaded from various sources such as CSV files, databases, or APIs. It is essential to ensure that the data is loaded correctly and that any initial preprocessing steps, such as handling missing values or encoding categorical variables, are performed accurately to prepare the data for further analysis.\n",
    "</di>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of anime_data:\n",
      "   anime_id                              name  \\\n",
      "0     32281                    Kimi no Na wa.   \n",
      "1      5114  Fullmetal Alchemist: Brotherhood   \n",
      "2     28977                          GintamaÂ°   \n",
      "3      9253                       Steins;Gate   \n",
      "4      9969                     Gintama&#039;   \n",
      "\n",
      "                                               genre   type episodes  rating  \\\n",
      "0               Drama, Romance, School, Supernatural  Movie        1    9.37   \n",
      "1  Action, Adventure, Drama, Fantasy, Magic, Mili...     TV       64    9.26   \n",
      "2  Action, Comedy, Historical, Parody, Samurai, S...     TV       51    9.25   \n",
      "3                                   Sci-Fi, Thriller     TV       24    9.17   \n",
      "4  Action, Comedy, Historical, Parody, Samurai, S...     TV       51    9.16   \n",
      "\n",
      "   members  \n",
      "0   200630  \n",
      "1   793665  \n",
      "2   114262  \n",
      "3   673572  \n",
      "4   151266  \n",
      "First 5 rows of submission_data:\n",
      "           ID  rating\n",
      "0  14862_1232     5.3\n",
      "1  14862_7974     4.0\n",
      "2  24873_2349     6.0\n",
      "First 5 rows of train_data:\n",
      "   user_id  anime_id  rating\n",
      "0        1     11617      10\n",
      "1        1     11757      10\n",
      "2        1     15451      10\n",
      "3        2     11771      10\n",
      "4        3        20       8\n",
      "\n",
      "First 5 rows of test_data:\n",
      "   user_id  anime_id\n",
      "0    40763     21405\n",
      "1    68791     10504\n",
      "2    40487      1281\n",
      "3    55290       165\n",
      "4    72323     11111\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "anime_data = pd.read_csv('anime.csv')\n",
    "submission_data = pd.read_csv('submission.csv')\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Display the first 5 rows of each dataset\n",
    "print(\"First 5 rows of anime_data:\")\n",
    "print(anime_data.head())\n",
    "\n",
    "print(\"First 5 rows of submission_data:\")\n",
    "print(submission_data.head())\n",
    "\n",
    "print(\"First 5 rows of train_data:\")\n",
    "print(train_data.head())\n",
    "\n",
    "print(\"\\nFirst 5 rows of test_data:\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning <a id=\"chapter4\"><a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <b>Data cleaning</b> is a crucial step in the data analysis process, involving the correction or removal of incorrect, corrupted, duplicate, or incomplete data within a dataset. Through various techniques such as filling missing values, removing outliers, and standardizing data formats, it ensures the accuracy and reliability of subsequent analyses and decision-making.\n",
    "</div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the anime_data: (12294, 7)\n",
      "The shape of the submission_data: (3, 2)\n",
      "The shape of the train_data: (5703555, 3)\n",
      "The shape of the test_data: (633686, 2)\n"
     ]
    }
   ],
   "source": [
    "# Get the shape of the anime_data DataFrame\n",
    "print('The shape of the anime_data:', anime_data.shape)\n",
    "\n",
    "# Get the shape of the submission_data DataFrame\n",
    "print('The shape of the submission_data:', submission_data.shape)\n",
    "\n",
    "# Get the shape of the train_data DataFrame\n",
    "print('The shape of the train_data:', train_data.shape)\n",
    "\n",
    "# Get the shape of the test_data DataFrame\n",
    "print('The shape of the test_data:', test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exploring the shape of the anime_data, submission_data, train_data and the test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12294 entries, 0 to 12293\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   anime_id  12294 non-null  int64  \n",
      " 1   name      12294 non-null  object \n",
      " 2   genre     12232 non-null  object \n",
      " 3   type      12269 non-null  object \n",
      " 4   episodes  12294 non-null  object \n",
      " 5   rating    12064 non-null  float64\n",
      " 6   members   12294 non-null  int64  \n",
      "dtypes: float64(1), int64(2), object(4)\n",
      "memory usage: 672.5+ KB\n",
      "The summary information of the anime_data: None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3 entries, 0 to 2\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   ID      3 non-null      object \n",
      " 1   rating  3 non-null      float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 180.0+ bytes\n",
      "The summary information of the submission_data: None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5703555 entries, 0 to 5703554\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Dtype\n",
      "---  ------    -----\n",
      " 0   user_id   int64\n",
      " 1   anime_id  int64\n",
      " 2   rating    int64\n",
      "dtypes: int64(3)\n",
      "memory usage: 130.5 MB\n",
      "The summary information of the train_data: None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 633686 entries, 0 to 633685\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count   Dtype\n",
      "---  ------    --------------   -----\n",
      " 0   user_id   633686 non-null  int64\n",
      " 1   anime_id  633686 non-null  int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 9.7 MB\n",
      "The summary information of the test_data: None\n"
     ]
    }
   ],
   "source": [
    "# Display summary information of the anime_data DataFrame\n",
    "print('The summary information of the anime_data:', anime_data.info())\n",
    "\n",
    "# Display summary information of the submission_data DataFrame\n",
    "print('The summary information of the submission_data:', submission_data.info())\n",
    "\n",
    "# Display summary information of the train_data DataFrame\n",
    "print('The summary information of the train_data:', train_data.info())\n",
    "\n",
    "# Display summary information of the test_data DataFrame\n",
    "print('The summary information of the test_data:', test_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Displaying summary information about the anime_data, submission_data, train_data and the test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime Data Null Values:\n",
      "genre has 62 null values\n",
      "type has 25 null values\n",
      "rating has 230 null values\n",
      "\n",
      "Submission Data Null Values:\n",
      "There are no null values in the DataFrame\n",
      "\n",
      "Train Data Null Values:\n",
      "There are no null values in the DataFrame\n",
      "\n",
      "Test Data Null Values:\n",
      "There are no null values in the DataFrame\n"
     ]
    }
   ],
   "source": [
    "# counting Null value\n",
    "def check_null_values(df):\n",
    "    \"\"\"\n",
    "    Print the count of null values for each column in a DataFrame.\n",
    "\n",
    "    This function iterates through each column in the DataFrame to check for the presence of null values.\n",
    "    If a column contains null values, it prints the column name along with the number of null values.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The pandas DataFrame to check for null values.\n",
    "\n",
    "    Returns:\n",
    "    None: This function does not return a value; it only prints information.\n",
    "    \"\"\"\n",
    "    has_nulls = False\n",
    "    for column in df:\n",
    "        null_count = df[column].isnull().sum()\n",
    "        if null_count > 0:\n",
    "            print(f'{column} has {null_count} null values')\n",
    "            has_nulls = True\n",
    "    \n",
    "    if not has_nulls:\n",
    "        print('There are no null values in the DataFrame')\n",
    "\n",
    "# Checking null values for each DataFrame\n",
    "print(\"Anime Data Null Values:\")\n",
    "check_null_values(anime_data)\n",
    "print(\"\\nSubmission Data Null Values:\")\n",
    "check_null_values(submission_data)\n",
    "print(\"\\nTrain Data Null Values:\")\n",
    "check_null_values(train_data)\n",
    "print(\"\\nTest Data Null Values:\")\n",
    "check_null_values(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values in 'rating' with a specific value like the mean rating\n",
    "mean_rating = anime_data['rating'].mean()\n",
    "anime_data['rating'] = anime_data['rating'].fillna(mean_rating)\n",
    "\n",
    "# Fill 'genre' and 'type' with the most common values (mode)\n",
    "most_common_genre = anime_data['genre'].mode()[0]\n",
    "most_common_type = anime_data['type'].mode()[0]\n",
    "\n",
    "anime_data['genre'] = anime_data['genre'].fillna(most_common_genre)\n",
    "anime_data['type'] = anime_data['type'].fillna(most_common_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anime Data Null Values:\n",
      "There are no null values in the DataFrame\n"
     ]
    }
   ],
   "source": [
    "# Checking null values for Anime DataFrame after filling the null values\n",
    "print(\"Anime Data Null Values:\")\n",
    "check_null_values(anime_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of duplicate rows in the anime_data: 0\n",
      "The number of duplicate rows in the submission_data: 0\n",
      "The number of duplicate rows in the train_data: 1\n",
      "The number of duplicate rows in the test_data: 0\n"
     ]
    }
   ],
   "source": [
    "# counting duplicate value\n",
    "def count_duplicate_rows(df):\n",
    "    \"\"\"\n",
    "    Count the number of duplicate rows in a DataFrame.\n",
    "\n",
    "    This function calculates the total number of duplicate rows in the DataFrame by calling the `duplicated` method,\n",
    "    which marks duplicates as `True`, and then sums these cases.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to check for duplicates.\n",
    "\n",
    "    Returns:\n",
    "    int: The count of duplicate rows.\n",
    "    \"\"\"\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    return duplicate_count\n",
    "\n",
    "# Counting duplicate rows for each DataFrame\n",
    "anime_duplicates = count_duplicate_rows(anime_data)\n",
    "submission_duplicates = count_duplicate_rows(submission_data)\n",
    "train_duplicates = count_duplicate_rows(train_data)\n",
    "test_duplicates = count_duplicate_rows(test_data)\n",
    "\n",
    "# Printing the count of duplicate rows\n",
    "print(f\"The number of duplicate rows in the anime_data: {anime_duplicates}\")\n",
    "print(f\"The number of duplicate rows in the submission_data: {submission_duplicates}\")\n",
    "print(f\"The number of duplicate rows in the train_data: {train_duplicates}\")\n",
    "print(f\"The number of duplicate rows in the test_data: {test_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All duplicated rows in train_data, including the first occurrence:\n",
      "         user_id  anime_id  rating\n",
      "3273846    42653     16498       8\n",
      "3273897    42653     16498       8\n"
     ]
    }
   ],
   "source": [
    "# View all rows that are duplicated, including the first occurrence\n",
    "all_duplicated_rows = train_data[train_data.duplicated(keep=False)]\n",
    "\n",
    "# Display all duplicated rows, including the first occurrence\n",
    "print(\"\\nAll duplicated rows in train_data, including the first occurrence:\")\n",
    "print(all_duplicated_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data after removing duplicate rows:\n",
      "(5703554, 3)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows and keep the first occurrence\n",
    "train_data = train_data.drop_duplicates(keep='first')\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(\"train_data after removing duplicate rows:\")\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Shape:\n",
    "- `anime_data` : 12294 rows and 7 columns.\n",
    "- `submission_data` : 3 rows and 2 columns.\n",
    "- `train_data` : 5703555 rows and 3 columns.\n",
    "- `test_data` : 633686 rows and 2 columns.\n",
    "2. Data Types:\n",
    "\n",
    "- `anime_data` :\n",
    "\n",
    "  - anime_id: int64\n",
    "  - name: object\n",
    "  - genre: object\n",
    "  - type: object\n",
    "  - episodes: object\n",
    "  - rating: float64\n",
    "  - members: int64\n",
    "- `submission_data` :\n",
    "\n",
    "  - ID: object\n",
    "  - rating: float64\n",
    "- `train_data` :\n",
    "\n",
    "  - user_id: int64\n",
    "  - anime_id: int64\n",
    "  - rating: int64\n",
    "- `test_data` :\n",
    "\n",
    "  - user_id: int64\n",
    "  - anime_id: int64\n",
    "3. Null Values:\n",
    "- `anime_data` :\n",
    "\n",
    "   - genre has 62 null values\n",
    "   - type has 25 null values\n",
    "   - rating has 230 null values\n",
    "   - submission_data: There are no null values.\n",
    "\n",
    "- `train_data` : There are no null values.\n",
    "\n",
    "- `test_data` : There are no null values.\n",
    "4. Duplicates:\n",
    "- `anime_data` : There are no duplicate rows.\n",
    "- `submission_data` : There are no duplicate rows.\n",
    "- `train_data` : There was 1 duplicate row, which has been removed. The shape of the cleaned train_data is now (5703554, 3).\n",
    "- `test_data` : There are no duplicate rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing <a id=\"chapter5\"><a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <b>Data preprocessing</b> is a fundamental step in the data analysis and machine learning pipeline, encompassing the transformation and organization of raw data into a format suitable for analysis. This process includes activities such as normalization, encoding categorical variables, feature scaling, and splitting data into training and testing sets, thereby enhancing the performance and accuracy of machine learning models.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) <a id=\"chapter6\"><a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exploratory Data Analysis (EDA)</b> is a vital step in the data analysis process, aimed at understanding the underlying patterns, relationships, and structure of the data. It involves various techniques such as visualizations, summary statistics, and correlation analysis to uncover insights and identify potential issues. EDA helps in forming hypotheses, guiding further analysis, and making informed decisions about data preprocessing and modeling.\n",
    "</di>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation <a id=\"chapter7\"><a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <b>Explanation:</b>  This step involves creating a mathemaical or computational model that categorize data into different classes or categories based on input features.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters and Hyperparameters tunning <a id=\"chapter8\"><a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Explanation </b> Fine-tuning model parameters and hyperparameters is a crucial step in the machine learning workflow aimed at optimizing the performance of a model. Here are the primary purposes and benefits of fine-tuning:\n",
    " \n",
    "* Improve Model Accuracy\n",
    "* prevent Overfitting and Underfitting \n",
    "* Enhance Model Efficiency\n",
    "* Adapt to Specific Data Characteristics \n",
    "* Improve Interpretability and Usability, and \n",
    "* Maximize Utility of Hyperparameters. \n",
    "</di>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a id=\"chapter9\"><a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "  <b>Conclusion:</b> The conclusion of a data analysis project ties together the findings and insights gained from the data. It reflects on the effectiveness of the preprocessing and cleaning steps, discusses the accuracy and reliability of the models used, and provides recommendations based on the results. A strong conclusion not only summarizes key points but also suggests future directions for research or improvements, emphasizing the overall impact and value of the analysis.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
